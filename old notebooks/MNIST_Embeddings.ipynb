{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45fb7df",
   "metadata": {},
   "source": [
    "# MNIST Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1598eb",
   "metadata": {},
   "source": [
    "## Imports and Hyper-parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3336f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/praveens/Desktop/synthetic_biometrics/visualize_embeddings\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import os \n",
    "import numpy as np \n",
    "from torchvision import transforms \n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import models \n",
    "\n",
    "print(os.getcwd())\n",
    "root = os.getcwd()\n",
    "\n",
    "from DeepFeatures import DeepFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea592aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "DATA_FOLDER = root + '/MNIST'\n",
    "IMAGES_FOLDER = root + '/Outputs/MNIST/Images'\n",
    "EMBEDS_FOLDER = root + '/Outputs/MNIST/Embeds'\n",
    "TENSORBOARD_FOLDER = root + '/Outputs/Tensorboard'\n",
    "EXPERIMENT_NAME = 'mnist_embeds_vgg16'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309520a",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a605168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(tensor, times=3):\n",
    "    return (torch.cat([tensor]*times, dim=0))\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize((221, 221)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "                                      stack\n",
    "                                    ])\n",
    "\n",
    "mnist_data = MNIST(root=r'./MNIST',\n",
    "                   download=False,\n",
    "                   transform=transformations)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e09acac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveens/miniconda3/envs/viz_emb/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/home/praveens/miniconda3/envs/viz_emb/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = models.vgg16(pretrained=True).to(device)\n",
    "\n",
    "# class Identity(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Identity, self).__init__()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return x\n",
    "\n",
    "vgg16.classifier = vgg16.classifier[0:4]\n",
    "vgg16.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fc033",
   "metadata": {},
   "source": [
    "## Initialize Tensorboard Logging Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fab1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_features = DeepFeatures(model=vgg16, \n",
    "                             images_folder=IMAGES_FOLDER, \n",
    "                             embeds_folder=EMBEDS_FOLDER, \n",
    "                             tensorboard_folder=TENSORBOARD_FOLDER, \n",
    "                             experiment_name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17b78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5affde8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = deep_features.generate_embeddings(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c480798c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0170163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = torch.zeros(4096)\n",
    "class1 = torch.tensor(6)\n",
    "class1_count = 0\n",
    "\n",
    "x2 = torch.zeros(4096)\n",
    "class2 = torch.tensor(1)\n",
    "class2_count = 0\n",
    "\n",
    "for index, (image, label) in enumerate(zip(images, labels)):\n",
    "    if label == class1:\n",
    "        x1 += e[index]\n",
    "        class1_count += 1\n",
    "    if label == class2:\n",
    "        x2 += e[index]\n",
    "        class2_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0151e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1/class1_count\n",
    "x2 = x2/class2_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe28366",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (x1/torch.norm(x1)) - (x2/torch.norm(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8870515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288b56b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e988d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, R = torch.linalg.qr(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "935cf221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4bcf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_inv = Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c3d3eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a988b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.eye(4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbce478d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0] = 0\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd961af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=4096, bias=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.classifier[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2c6760b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.7.weight\n",
      "features.7.bias\n",
      "features.10.weight\n",
      "features.10.bias\n",
      "features.12.weight\n",
      "features.12.bias\n",
      "features.14.weight\n",
      "features.14.bias\n",
      "features.17.weight\n",
      "features.17.bias\n",
      "features.19.weight\n",
      "features.19.bias\n",
      "features.21.weight\n",
      "features.21.bias\n",
      "features.24.weight\n",
      "features.24.bias\n",
      "features.26.weight\n",
      "features.26.bias\n",
      "features.28.weight\n",
      "features.28.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.3.weight\n",
      "classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in vgg16.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7669ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = list(vgg16.parameters())[-2]\n",
    "last_bias = list(vgg16.parameters())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92d802f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a424008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = last_layer*Q*S*Q_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec519681",
   "metadata": {},
   "source": [
    "## Write Embeddings to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d718872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_images, batch_labels = next(iter(data_loader))\n",
    "deep_features.write_embeddings(x=batch_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1785d592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveens/Desktop/synthetic_biometrics/visualize_embeddings/DeepFeatures.py:94: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /home/builder/mc3/envs/pytorch-build/envs/pytorch-build/conda-bld/pytorch_1673601922403/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  all_embeds = torch.Tensor(all_embeds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4096])\n",
      "torch.Size([64, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "deep_features.create_tensorboard_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d670fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
